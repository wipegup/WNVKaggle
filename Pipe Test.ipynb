{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#class ChicagoPreprocessor(object):#\n",
    "#    def __init__(train, test = None):\n",
    "#        self.parkFinder =\n",
    "\n",
    "\n",
    "def agg_on_species(train, test):\n",
    "    \n",
    "    noAgg = [c for c in train.columns if c not in ['NumMosquitos','WnvPresent']]\n",
    "\n",
    "    agg = train.groupby(noAgg)['NumMosquitos', 'WnvPresent'].sum()\n",
    "\n",
    "    for i, c in enumerate(noAgg):\n",
    "        agg[c] = agg.index.map(lambda x:x[i])\n",
    "\n",
    "    agg.index = range(0,len(agg))\n",
    "    agg['WnvPresent'] = (agg['WnvPresent'].map(lambda x:x>0)).astype(int)\n",
    "    return agg, test\n",
    "\n",
    "def InitPrepross(train, test):\n",
    "\n",
    "    def location_add(df):\n",
    "        df['Location'] = [(df.loc[idx,'Longitude'], df.loc[idx, 'Latitude'])\n",
    "                            for idx in df.index]\n",
    "        return df\n",
    "\n",
    "    def change_date(df):\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def drop_unused(df):\n",
    "        for col in ['Address','Block','Street',\n",
    "              'AddressNumberAndStreet', 'AddressAccuracy',\n",
    "                    ]:\n",
    "            try:\n",
    "                df = df.drop(col, axis = 'columns')\n",
    "            except:\n",
    "                print(col, 'not present')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def species_dummies(df):\n",
    "        species = ['CULEX PIPIENS', 'CULEX PIPIENS/RESTUANS',\n",
    "                'CULEX RESTUANS', 'CULEX SALINARIUS',\n",
    "                'CULEX TERRITANS', 'CULEX TARSALIS',\n",
    "                 'CULEX ERRATICUS']\n",
    "        for s in species:\n",
    "            df[s] = (df['Species'] == s).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(df):\n",
    "        df = drop_unused(df)\n",
    "        df = location_add(df)\n",
    "        df = change_date(df)\n",
    "        df = species_dummies(df)\n",
    "        return df\n",
    "        \n",
    "    return transform(train), transform(test)\n",
    "\n",
    "def LocationProcess(train, test):\n",
    "    parkDir = './AddData/Parks/'\n",
    "    waterDir = './AddData/Water/'\n",
    "\n",
    "    def buildWaterFinder():\n",
    "        water = [f for f in listdir(waterDir)\n",
    "                if isfile(join(waterDir,f))\n",
    "                if f.count('.csv') ==0]\n",
    "\n",
    "        waterShape = waterDir + water[0].split('.')[0]\n",
    "        waterSR = shapefile.Reader(waterShape).shapeRecords()\n",
    "\n",
    "        waterFinder = {}\n",
    "        for i, s in enumerate(waterSR):\n",
    "            waterFinder[i] = cKDTree(s.shape.points)\n",
    "\n",
    "        return waterFinder\n",
    "\n",
    "    def buildParkDicts():\n",
    "        parks = [f for f in listdir(parkDir)\n",
    "                if isfile(join(parkDir,f))\n",
    "                if f.count('.csv') ==0]\n",
    "        parkShape = parkDir + parks[0].split('.')[0]\n",
    "        parkSR = shapefile.Reader(parkShape).shapeRecords()\n",
    "\n",
    "        parkFinder = {}\n",
    "        parkSize = {}\n",
    "        for s in parkSR:\n",
    "            parkSize[s.record[4]] = s.record[19]\n",
    "            parkFinder[s.record[4]] = cKDTree(s.shape.points)\n",
    "\n",
    "        return parkFinder, parkSize\n",
    "\n",
    "    def calculate_distances(loc, finder, size = None):\n",
    "        Dist = {}\n",
    "        for k in finder:\n",
    "            Dist[k] = finder[k].query(loc, 1)[0]\n",
    "\n",
    "        if size:\n",
    "            toRet = {}\n",
    "            for k in Dist:\n",
    "                Dist[k] = (Dist[k], size[k], size[k]/(Dist[k]**2))\n",
    "        return Dist\n",
    "\n",
    "    def dfFromDict(dct):\n",
    "        toRet = pd.DataFrame(dct)\n",
    "        toRet = toRet.transpose()\n",
    "        toRet.index = [idx for idx in toRet.index]\n",
    "\n",
    "        if type(toRet.iloc[0,0]) == tuple:\n",
    "            for c in toRet:\n",
    "                toRet['P ' + str(c) + ' A'] = [e[1] for e in toRet[c]]\n",
    "                toRet['P ' + str(c) + ' E'] = [e[2] for e in toRet[c]]\n",
    "                toRet['P ' + str(c)] = [e[0] for e in toRet[c]]\n",
    "                toRet = toRet.drop(c, axis = 'columns')\n",
    "        else:\n",
    "            toRet.columns = ['W ' + str(c) for c in toRet.columns]\n",
    "            \n",
    "        return toRet\n",
    "\n",
    "    def info(df, finder, size = None):\n",
    "        uniqueLocs = df['Location'].unique()\n",
    "        rows = {}\n",
    "        for loc in uniqueLocs:\n",
    "            rows[loc] = calculate_distances(loc, finder, size)\n",
    "\n",
    "        return dfFromDict(rows)\n",
    "\n",
    "    def transform(df):\n",
    "        toRet = pd.concat( [info(df, waterFinder),\n",
    "                    info(df, parkFinder, parkSize)],\n",
    "                    axis = 'columns')\n",
    "\n",
    "        return toRet\n",
    "\n",
    "    parkFinder, parkSize = buildParkDicts()\n",
    "    waterFinder = buildWaterFinder()\n",
    "\n",
    "    # Returns DFs: index = locations\n",
    "    return transform(train), transform(test)\n",
    "\n",
    "def SVD(train, test):\n",
    "\n",
    "    def find_cols(df, tpe):\n",
    "        mask = [c for c in df.columns if c[0] == tpe ]\n",
    "\n",
    "        return df.loc[:,mask]\n",
    "\n",
    "    def yeildFitTSVD(df):\n",
    "        comps = 4\n",
    "\n",
    "        TSVD = TruncatedSVD(n_components = comps)\n",
    "        TSVD.fit(df)\n",
    "\n",
    "        return TSVD\n",
    "\n",
    "    def transformTSVD(df, TSVD,tpe):\n",
    "        toRet = TSVD.transform(df)\n",
    "        toRet = pd.DataFrame(toRet, index = df.index)\n",
    "        toRet.columns = [tpe+str(c) for c in toRet.columns]\n",
    "        \n",
    "        return toRet\n",
    "\n",
    "    \n",
    "    toRetTrain = []\n",
    "    toRetTest = []\n",
    "    for t in ['W', 'P']:\n",
    "        sTrain = find_cols(train, t)\n",
    "        sTest = find_cols(test,t)\n",
    "        sTSVD = yeildFitTSVD(sTrain)\n",
    "\n",
    "        toRetTrain.append(transformTSVD(sTrain, sTSVD, t))\n",
    "        toRetTest.append(transformTSVD(sTest, sTSVD, t))\n",
    "\n",
    "    toRetTrain = pd.concat(toRetTrain, axis = 'columns')\n",
    "    toRetTest = pd.concat(toRetTest, axis = 'columns')\n",
    "\n",
    "    return toRetTrain, toRetTest\n",
    "\n",
    "def WeatherProcess(train, test):\n",
    "\n",
    "    def yeildWeather(target):\n",
    "        weather = pd.read_csv(target)\n",
    "        weather['Date'] = pd.to_datetime(weather['Date'])\n",
    "\n",
    "        toDrop = ['Depart', 'Depth','Water1',\n",
    "                'SnowFall', 'CodeSum', 'Heat',\n",
    "                'Cool', 'Sunrise']\n",
    "        weather = weather.drop(toDrop, axis=1)\n",
    "\n",
    "        toReplace = {'M':np.nan, '  T': 0.001, '-': '0000'}\n",
    "        for k in toReplace:\n",
    "            weather = weather.replace(k, toReplace[k])\n",
    "\n",
    "\n",
    "        toFloats = ['Tavg', 'WetBulb', 'PrecipTotal','StnPressure',\n",
    "                    'SeaLevel', 'ResultSpeed','AvgSpeed']\n",
    "        for c in toFloats:\n",
    "            weather[c] = weather[c].astype(float)\n",
    "\n",
    "        weather['Sunset'] = [date\n",
    "                            if date[-2:] != '60'\n",
    "                            else str(int(date[0:2])+1)+'00'\n",
    "                            for date in weather['Sunset']]\n",
    "\n",
    "        weather['Sunset'] = pd.to_datetime(weather['Sunset'],\n",
    "                                            format=\"%H%M\")\n",
    "        weather.dropna(inplace=True)\n",
    "\n",
    "        return weather[weather['Station']== 1]\n",
    "\n",
    "    def yeildAvgTemp(weather):\n",
    "        weather['Wk'] = weather['Date'].dt.week\n",
    "        weekTemp = pd.DataFrame(\n",
    "                        weather.groupby('Wk')['Tavg'].mean())\n",
    "        weekTemp['Week'] = weekTemp.index - 17\n",
    "        weekTemp['Week^2'] = weekTemp['Week']**2\n",
    "\n",
    "        lr = LinearRegression().fit(weekTemp.drop('Tavg', axis = 'columns'),\n",
    "                                    weekTemp['Tavg'])\n",
    "        toRet = {}\n",
    "        for w in range(1,53):\n",
    "            toRet[w] = lr.intercept_ + (lr.coef_[0]*(w-17)) + (lr.coef_[1] * ((w-17)**2))\n",
    "\n",
    "        return toRet\n",
    "\n",
    "    def calculate_agregate( weather_sub, avgTDict):\n",
    "        toRet = pd.Series()\n",
    "\n",
    "        allAgg = [np.max, np.min, np.mean]\n",
    "        toAgg = {'DewPoint': allAgg,\n",
    "                'StnPressure': allAgg,\n",
    "                'AvgSpeed': allAgg,\n",
    "                'Tmax':[np.max],\n",
    "                'Tmin':[np.min],\n",
    "                'Tavg':[np.mean],\n",
    "                'PrecipTotal':[np.sum, np.mean]\n",
    "                }\n",
    "        for k in toAgg:\n",
    "            for f in toAgg[k]:\n",
    "                toRet.loc[k + str(f).split(' ')[1]] = f(weather_sub[k])\n",
    "\n",
    "        finalEntry = weather_sub.iloc[len(weather_sub)-1]\n",
    "\n",
    "        toRet['temp_expected'] = avgTDict[pd.to_datetime(finalEntry['Date']).week]\n",
    "        toRet['temp_diff'] = toRet['Tavgmean'] - toRet['temp_expected']\n",
    "\n",
    "        sunset = finalEntry['Sunset']\n",
    "        toRet['sunset'] = sunset.hour + (sunset.minute / 60)\n",
    "\n",
    "        return toRet\n",
    "\n",
    "    def date_ranges(dates):\n",
    "        uniqueYears = set([pd.to_datetime(d).year for d in dates])\n",
    "\n",
    "        dates = sorted(dates)\n",
    "        fyear = []\n",
    "        for y in uniqueYears:\n",
    "            for d in dates:\n",
    "                if pd.to_datetime(d).year == y:\n",
    "                    fyear.append(d)\n",
    "                    break\n",
    "\n",
    "        for d in fyear:\n",
    "            dates = np.insert(dates, 0, d - pd.Timedelta(days = 8))\n",
    "\n",
    "        dateRanges = []\n",
    "        for i in range(len(dates)-1):\n",
    "            if pd.to_datetime(dates[i]).year == pd.to_datetime(dates[i+1]).year:\n",
    "                dateRanges.append( (dates[i], dates[i+1]) )\n",
    "\n",
    "        return dateRanges\n",
    "\n",
    "    def subset_weather(dateRange, weather):\n",
    "        mask = (weather['Date']>dateRange[0]) & (weather['Date'] <= dateRange[1])\n",
    "        return weather.loc[mask]\n",
    "\n",
    "    def TWeatherDFMaker(dct):\n",
    "        toRet = pd.DataFrame().from_dict(dct)\n",
    "        toRet = toRet.transpose()\n",
    "        toRet.index = [idx for idx in toRet.index]\n",
    "        toRet['Trap'] = toRet.index.map(lambda x: x[0])\n",
    "        toRet['Date'] = toRet.index.map(lambda x: x[1])\n",
    "        toRet.index = range(len(toRet))\n",
    "\n",
    "        return toRet\n",
    "\n",
    "    def trap_agregator(trap_df, weather, avgTDict):\n",
    "        trapWeather = {}\n",
    "        trap = trap_df['Trap'].iloc[0]\n",
    "\n",
    "        dates = trap_df['Date'].unique()\n",
    "        dates = sorted(dates)\n",
    "\n",
    "        dateRanges = date_ranges(dates)\n",
    "\n",
    "        for dr in dateRanges:\n",
    "            weather_sub = subset_weather(dr, weather)\n",
    "            trapWeather[(trap, dr[1])] = calculate_agregate(weather_sub, avgTDict)\n",
    "        toRet = pd.DataFrame().from_dict(trapWeather)\n",
    "\n",
    "        return TWeatherDFMaker(trapWeather)\n",
    "\n",
    "    def transform(df):\n",
    "        observations = []\n",
    "\n",
    "        traps = df['Trap'].unique()\n",
    "        for t in traps:\n",
    "            observations.append(trap_agregator(df[df['Trap'] == t],\n",
    "            weather, avgTDict))\n",
    "        toRet = pd.concat(observations, axis = 'rows')\n",
    "\n",
    "        return toRet\n",
    "\n",
    "    weatherTarget = './input/weather.csv'\n",
    "    weather = yeildWeather(weatherTarget)\n",
    "    avgTDict = yeildAvgTemp(weather)\n",
    "\n",
    "    return transform(train), transform(test)\n",
    "\n",
    "def FeatUnion(train, test):\n",
    "    train, test = agg_on_species(train, test)\n",
    "    train, test = InitPrepross(train, test)\n",
    "\n",
    "    trainW, testW = WeatherProcess(train, test)\n",
    "\n",
    "    trainL, testL = LocationProcess(train, test)\n",
    "    trainL, testL = SVD(trainL, testL)\n",
    "\n",
    "    train = train.merge(trainW, on = ['Trap','Date'])\n",
    "    test = test.merge(testW,on = ['Trap','Date'])\n",
    "\n",
    "    train = train.merge(trainL, left_on = 'Location', right_index = True)\n",
    "    test = test.merge(testL, left_on = 'Location', right_index = True)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv', index_col= 0)\n",
    "\n",
    "tup = FeatUnion(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8078, 40)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tup[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weather' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\GA\\WNile\\GroupWestNile\\ProcessPipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweather\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'weather' is not defined"
     ]
    }
   ],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i ProcessPipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tup1 = ProcessPipeline(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
